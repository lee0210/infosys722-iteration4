{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3117901-874c-4979-9551-0b2f597f3ad4",
   "metadata": {},
   "source": [
    "## INFOSYS 722 Assignement Iteration 4 - Sub-iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4d98d-8000-4070-8d6e-d6d1501f4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edb78b-f681-48dd-ac1e-eb26458ddefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('infosys722-i4-i2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d7fa6-c616-4326-87b8-1f417f6d9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_purchase_detail = spark.read.options(compression='gzip').format('parquet').load(\n",
    "    './Ready Datasets/purchase_detail_v001')\n",
    "integrated_sales = spark.read.options(compression='gzip').format('parquet').load(\n",
    "    './Ready Datasets/sales_v001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70075e-ab29-46e4-904c-344b5e050496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extreme_by_z_scores(dataframe, column, threshold = 1):\n",
    "    mean, std = dataframe.agg(F.mean(column), F.stddev(column)).collect()[0]\n",
    "    dataframe = dataframe.withColumn('z_scores', (F.col(column) - mean) / std)\n",
    "    return dataframe.where(F.col('z_scores') <= threshold).drop('z_scores')\n",
    "\n",
    "integrated_sales = remove_extreme_by_z_scores(integrated_sales, 'SalesQuantity')\n",
    "integrated_sales.describe('SalesQuantity').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0e8e1-6adc-466c-bd0a-b2f593a14304",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae7a45-df9d-4e52-8be4-8d74ddf5551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes, GBTClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1c628-8981-40e6-83c1-d74ae246daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_model(result, labelCol, predictionCol='prediction', metrics = ['rmse', 'mse', 'mae', 'r2']):\n",
    "    results = [(metric, \n",
    "                RegressionEvaluator(labelCol=labelCol, metricName=metric, predictionCol=predictionCol).evaluate(result)) \n",
    "               for metric in metrics]\n",
    "    for metric, result in results:\n",
    "        print(f'{metric}: {result}')\n",
    "    return dict(results)\n",
    "\n",
    "def evaluate_classification_model(result, labelCol, predictionCol='prediction', metrics = ['f1', 'accuracy']):\n",
    "    results = [(metric, \n",
    "                MulticlassClassificationEvaluator(\n",
    "                    labelCol=labelCol, metricName=metric, predictionCol=predictionCol).evaluate(result)) \n",
    "               for metric in metrics]\n",
    "    for metric, v in results:\n",
    "        print(f'{metric}: {v}')\n",
    "    return dict(results)\n",
    "    \n",
    "def get_string_indexers(dataframe, suffix='_index'):\n",
    "    indexers = []\n",
    "    for feature, dtype in dataframe.dtypes:\n",
    "        if dtype == 'string':\n",
    "            indexers.append(StringIndexer(inputCol=feature, outputCol=feature + suffix))\n",
    "    return indexers\n",
    "\n",
    "def generate_baseline(dataframe, target):\n",
    "    target_mean = dataframe.agg(F.mean(target)).collect()[0][0]\n",
    "    return dataframe.withColumn('baseline_prediction', F.lit(target_mean))\n",
    "\n",
    "def balance(dataframe, target, size=None):\n",
    "    categories = dataframe.groupBy(target).agg(F.count(target).alias('count'))\n",
    "    if size is None:\n",
    "        size = int(categories.agg(F.mean('count')).collect()[0][0])\n",
    "    balanced_df = spark.createDataFrame([], dataframe.schema)\n",
    "    for category, count in categories.collect():\n",
    "        if count > size:\n",
    "            sample = dataframe.where(F.col(target) == category).orderBy(F.rand(13)).limit(size)\n",
    "        else:\n",
    "            ratio = math.ceil(size/count)\n",
    "            sample = dataframe.where(F.col(target) == category).withColumn(\n",
    "                'dummy', F.explode(F.array([F.lit(i) for i in range(ratio)]))).drop('dummy').orderBy(F.rand(13)).limit(size)\n",
    "        balanced_df = balanced_df.unionAll(sample)\n",
    "    return balanced_df\n",
    "\n",
    "def print_evaluation(model, train, test, target, transformer, evaluate_function=evaluate_regression_model):\n",
    "    print('Training data evaluation metrics:')\n",
    "    result = model.transform(transformer.transform(train))\n",
    "    evaluate_function(result, target)\n",
    "\n",
    "    print('Testing data evaluation metrics:')\n",
    "    result = model.transform(transformer.transform(test))\n",
    "    evaluate_function(result, target)\n",
    "\n",
    "def print_feature_importance(model, train, rounding=6):\n",
    "    attrs = train.schema[\"features\"].metadata['ml_attr']['attrs']\n",
    "    feature_names = []\n",
    "    for key, feature_list in attrs.items():\n",
    "        feature_names = feature_names + feature_list\n",
    "    feature_importances = [(f['name'], float(round(model.featureImportances[f['idx']], rounding))) for f in feature_names]\n",
    "    spark.createDataFrame(feature_importances, ['feature', 'importance']).orderBy('importance', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2ae86-8a69-400e-9ac3-c713dd787dfc",
   "metadata": {},
   "source": [
    "### Compare Algorithm Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e2d1-422a-4a39-83ec-cf7b860ca826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = integrated_sales.randomSplit([0.7, 0.3], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215070b-a573-4f11-8661-d7db566def1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train.toPandas()['SalesQuantity'], bins=[i+0.5 for i in range(0, 9)], rwidth=0.5, alpha=0.5, label='train')\n",
    "balanced_train = balance(train, 'SalesQuantity')\n",
    "plt.hist(balanced_train.toPandas()['SalesQuantity'], bins=[i+0.5 for i in range(0, 9)], rwidth=0.5, alpha=0.5, label='balanced')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b579b72-7cb0-4dd4-99a5-e0a66ce47070",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = balanced_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fda14f-f264-4faa-8c6c-dd95e0fcedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_pipeline = Pipeline(stages=get_string_indexers(integrated_sales) + [\n",
    "    VectorAssembler(inputCols=[\n",
    "        'SalesPrice', 'DayOfMonth', 'PurchasePrice',\n",
    "        'VendorNumber_index', 'DayOfWeek_index'], \n",
    "                    outputCol='features')\n",
    "])\n",
    "transformer = transformation_pipeline.fit(integrated_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7b175-f13b-4a29-b2c7-db729ddac2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dtr = DecisionTreeClassifier(maxDepth=10, labelCol='SalesQuantity', featuresCol='features', maxBins=256)\n",
    "model = dtr.fit(transformer.transform(train))\n",
    "print_evaluation(model, train, test, 'SalesQuantity', transformer, evaluate_function=evaluate_classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad24a3-bd95-4e3b-abb6-7505dfb51748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rfr = RandomForestClassifier(maxDepth=10, labelCol='SalesQuantity', featuresCol='features', maxBins=256)\n",
    "model = rfr.fit(transformer.transform(train))\n",
    "print_evaluation(model, train, test, 'SalesQuantity', transformer, evaluate_function=evaluate_classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e8cd7-1e51-401e-a98a-526bf244c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb = NaiveBayes(labelCol='SalesQuantity', featuresCol='features')\n",
    "model = nb.fit(transformer.transform(train))\n",
    "print_evaluation(model, train, test, 'SalesQuantity', transformer, evaluate_function=evaluate_classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470eeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_pipeline = Pipeline(stages=get_string_indexers(integrated_sales) + [\n",
    "    VectorAssembler(inputCols=[\n",
    "        'SalesPrice_log10', 'DayOfMonth', 'PurchasePrice_log10',\n",
    "        'VendorNumber_index', 'DayOfWeek_index'], \n",
    "                    outputCol='features')\n",
    "])\n",
    "transformer = transformation_pipeline.fit(integrated_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff25790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dtr = DecisionTreeClassifier(maxDepth=10, labelCol='SalesQuantity', featuresCol='features', maxBins=256)\n",
    "model = dtr.fit(transformer.transform(train))\n",
    "print_evaluation(model, train, test, 'SalesQuantity', transformer, evaluate_function=evaluate_classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81274a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rfr = RandomForestClassifier(maxDepth=10, labelCol='SalesQuantity', featuresCol='features', maxBins=256)\n",
    "model = rfr.fit(transformer.transform(train))\n",
    "print_evaluation(model, train, test, 'SalesQuantity', transformer, evaluate_function=evaluate_classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfd0ef-fa22-4323-b4e0-47500e5ca8c6",
   "metadata": {},
   "source": [
    "### Data Mining - Sales Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdce2a-e23f-4e3c-b7ee-16bda8b2a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "transformation_pipeline = Pipeline(stages=get_string_indexers(integrated_sales) + [\n",
    "    VectorAssembler(inputCols=[\n",
    "        'SalesPrice', 'DayOfMonth', 'PurchasePrice',\n",
    "        'VendorNumber_index', 'DayOfWeek_index'], \n",
    "                    outputCol='features')\n",
    "])\n",
    "transformer = transformation_pipeline.fit(integrated_sales)\n",
    "\n",
    "test_data_size = 1 / (math.sqrt(5) + 1)\n",
    "print(f'Test data size is {test_data_size}')\n",
    "train, test = integrated_sales.randomSplit([1-test_data_size, test_data_size], seed=13)\n",
    "train = balance(train, 'SalesQuantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540eddb0-1b7c-442c-b01e-afefa5eae151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train.toPandas()['SalesQuantity'], bins=[i+0.5 for i in range(0, 9)], rwidth=0.5, alpha=0.5, label='balanced')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0505f-a81e-4f91-84e7-b3ded6708853",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def try_depth(max_depth):\n",
    "    if cache.get(max_depth) is not None:\n",
    "        return cache.get(max_depth)\n",
    "    print('===============================================')\n",
    "    print(f'Trying maxDepth: {max_depth}')\n",
    "    rfr = RandomForestClassifier(maxDepth=max_depth, labelCol='SalesQuantity', featuresCol='features', maxBins=256)\n",
    "    model = rfr.fit(transformer.transform(train))\n",
    "    result = model.transform(transformer.transform(test))\n",
    "    cache[max_depth] = evaluate_classification_model(result, 'SalesQuantity')\n",
    "    return cache[max_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806665a2-9856-418b-85bc-39b780a0e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_max_depth(left, right):\n",
    "    if left == right:\n",
    "        return left\n",
    "    left_metric, right_metric = try_depth(left), try_depth(right)\n",
    "    if left_metric['accuracy'] < right_metric['accuracy']:\n",
    "        middle = math.ceil((left + right) / 2)\n",
    "        return find_best_max_depth(middle, right)\n",
    "    else:\n",
    "        middle = math.floor((left + right) / 2)\n",
    "        return find_best_max_depth(left, middle)\n",
    "\n",
    "find_best_max_depth(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79382fed-2f51-4a65-a07e-ae956a114943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
